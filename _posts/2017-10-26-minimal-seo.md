---
layout: post
title: "[Заметка] Минимальный SEO-набор"
date: '2017-10-26'
category:
  - it
  - seo
---
<!--more-->
Чтобы улучшить поиск Яндекса по сайту, необходимо следовать правилам:
  - Запретите в файле robots.txt индексирование страниц, неинтересных пользователям.
  - Используйте мета-теги
  - Создайте файл sitemap с дополнительной информацией о страницах сайта, подлежащих индексированию.
  - Создайте страницу или несколько страниц, которые содержат ссылки на все документы сайта.
  - Настройте сервер на выдачу правильного HTTP-статуса (например, если страница не существует, сервер должен отправлять HTTP-статус 404 Not Found).

## Файл robots.txt
Я думаю, что любой разработчик однажды слышал про этот файл. Файл направляет поисковые системы.
В [стандарт исключений](http://www.robotstxt.org/robotstxt.html) входит всего две директивы:
  - `User-agent` - заголовочная директива, которая говорит о каких ботах речь, здесь звёздочка рассматривается как «все боты».
  - `Disallow` - какие страницы сайта не надо посещать.
Таким образом минимальный файл может выглядеть так:

{%- highlight text -%}
User-agent: Google      # Для бота Google
Disallow:               # Разрешено посещать любые страниц

User-agent: YandexBot   # Для бота Yandex
Disallow: /cgi-bin/     # Запрещено посещать /cgi-bin/ и дочерние
Disallow: /~dir/inner/  # Запрещено посещать /~dir/, кроме /dir/inner/

User-agent: *           # Для всех остальных
Disallow: /             # Запрещено посещать любые страницы сайта
{%- endhighlight -%}

Если бот встретит директиву, предназначеную для него, то читать `*` он не будет.
У Яндекс множество ботов, любой из них реагирует на директиву `User-agent: Yandex`. У этих ботов есть еще несколько директив:
  - `Allow` - разрешает просматривать страницы.
При совместном использовании `Disallow` и `Allow`, значения директив выстраиваются по длине от меньшего значения к большему, а затем применяются поочередно. Если длины одинаковы, то приоритет выше у `Allow`.

```text
# Исходный robots.txt:
User-agent: Yandex
Allow: /catalog
Disallow: /

# Сортированный robots.txt:
User-agent: Yandex
Disallow: /
Allow: /catalog
# разрешает скачивать только страницы,
# начинающиеся с '/catalog'
```

Отличительные особенности ботов от Яндекс:
  - Разрешается использовать `*` в любом месте `Disallow` и `Allow`.

```text
User-agent: Yandex
Disallow: /cgi-bin/*.aspx # запрещает '/cgi-bin/example.aspx'
                          # и '/cgi-bin/private/test.aspx'
Disallow: /*private       # запрещает не только '/private',
                          # но и '/cgi-bin/private'
```

  - Можно использовать `$`, чтобы ограничить значение директивы справа.

```text
User-agent: Yandex
Disallow: /example$       # запрещает '/example',
                          # но не запрещает '/example.html'
```

  - `sitemap` - полный путь до карты сайта. Директива может быть написана в любом месте файла, она все равно будет прочитана.

```text
sitemap: https://example.com/site_structure/my_sitemaps1.xml
```

  - `Host` - указатель на главное зеркало сайта, указывается сразу после `Disallow` и `Allow`. Используется первая указанная в файле. Обязательно нужно указать HTTPS, если используется, порт, если он отличается от 80.

```text
Host: https://www.myhost.ru
```

  - `Crawl-delay` - задает таймаут в секундах между загрузками страниц, так можно избежать нагрузки со стороны бота.
  - `Clean-param` - если страница принимает динамические параметры, которые не влияют на её содержимое, то можно описать эти параметры и они будут проигнорированы ботом. Параметр может быть указан в любом месте сайта, несколько параметров перечисляются через `&`.

```text
Clean-param: param1 /some_dir/some_page.php
  # Здесь все страницы с параметром param1 будут сведены к одной
  # www.example.com/some_dir/some_page.php?param1=site_1&param2=123
  # www.example.com/some_dir/some_page.php?param1=site_2&param2=123
  # www.example.com/some_dir/some_page.php?param1=site_3&param2=123
  # => www.example.com/some_dir/some_page.php?param1=site_1&param2=123
```

## Мета-теги и HTML-элементы
Мета-теги позволяют указывать метаданные о странице сайта. Ряд мета-тегов также используется роботами поисковых систем. Робот Яндекса учитывает мета-теги:
  - `<meta name="Keywords" content="..."/>` - ключевые слова.
  - `<meta name="Description" content="..."/>` - описание сайта.
  - `<meta name="viewport" content="width=device-width, initial-scale=1">` - адаптация для мобильных стройств, если тега не будет, то мобильная версия будет как на десктопе.
  - `<meta http-equiv="Content-Type" content="type; charset=..."/>` - тип и кодировка документа.
  - `<meta http-equiv="refresh" content="N;url=_адрес цели перенаправления_"/>` - редирект через N секунд.
  - Мета-теги указания для роботов (можно заменить `robots` на `yandex`):
    - `<meta name="robots" content="all"/>` - индексировать всё на странице.
    - `<meta name="robots" content="noindex"/>` - не индексировать текст страницы.
    - `<meta name="robots" content="nofollow"/>` - не ходить по ссылкам на странице.
    - `<meta name="robots" content="none"/>` - не индексировать текст, не ходить по ссылкам страницы.
    - `<meta name="robots" content="noarchive"/>` - не показывать ссылку, как сохраненную копию.
    - `<meta name="robots" content="noyaca"/>` - не использовать описание из Яндекс.Каталога для сниппета в результатах поиска.
С помощью элемента `<noindex>` можно запретить индексировать выбранный участок текста.
С помощью атрибута `rel="nofollow"` элемента `a` можно запретить проходить по данной ссылке.
`<link rel="canonical" href="http://www.example.com/blog"/>` указывает по какому адресу находится предпочитаемый адрес текущей страницы.

## Карта сайта sitemap
Sitemap — это файл со ссылками на страницы сайта, который сообщает поисковым системам об актуальной структуре сайта. Яндекс поддерживает форматы XML и TXT (не рекомендуется). Яндекс рекомендует использовать Sitemap, если на сайте:
  - большое количество страниц;
  - отдельные страницы без навигационных ссылок;
  - глубокая вложенность.

Разберем на примере:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
   <url>
      <loc>http://www.example.com/</loc> <!-- Адрес страницы -->
      <lastmod>2005-01-01</lastmod> <!-- Время последней модификации -->
      <changefreq>monthly</changefreq>
        <!--
          Частота обновлений
            always - каждый раз новая
            hourly
            daily
            weekly
            monthly
            yearly
            never - никогда не обновляется
        -->
      <priority>0.8</priority> <!-- Приоритет от 0.0 до 1.0. Очередность загрузки страниц роботом -->
   </url>

    <url>
       <loc>http://www.example.com/about/</loc>
       <lastmod>2006-01-01</lastmod>
       <changefreq>monthly</changefreq>
       <priority>0.8</priority>
    </url>
</urlset>
```
